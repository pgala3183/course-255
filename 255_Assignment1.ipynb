{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "X9HhPr0fAFt0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class WRMSSEEvaluator:\n",
        "    def __init__(self, sales_df, calendar_df, prices_df, weight_cache=None):\n",
        "        # sales_df: wide (rows=series, cols=d_1..d_T), plus id keys\n",
        "        self.sales_df = sales_df\n",
        "        self.calendar = calendar_df\n",
        "        self.prices = prices_df\n",
        "        self.weight_cache = weight_cache\n",
        "        self.levels = [\n",
        "            [\"all\"], [\"state_id\"], [\"store_id\"], [\"cat_id\"], [\"dept_id\"],\n",
        "            [\"state_id\",\"cat_id\"], [\"state_id\",\"dept_id\"],\n",
        "            [\"store_id\",\"cat_id\"], [\"store_id\",\"dept_id\"],\n",
        "            [\"item_id\"], [\"state_id\",\"item_id\"], [\"item_id\",\"store_id\"]\n",
        "        ]  # 12 levels [2]\n",
        "\n",
        "    def _build_aggregates(self, df_keys_vals):\n",
        "        # df_keys_vals: keys + d_ cols\n",
        "        aggs = []\n",
        "        for keys in self.levels:\n",
        "            if keys == [\"all\"]:\n",
        "                agg = df_keys_vals.filter(like=\"d_\").sum(axis=0).to_frame().T\n",
        "                agg[\"all\"] = \"Total\"\n",
        "            else:\n",
        "                agg = df_keys_vals.groupby(keys).sum(numeric_only=True).reset_index()\n",
        "            aggs.append((keys, agg))\n",
        "        return aggs  # list of (keys, df) [2]\n",
        "\n",
        "    def _scale_denominators(self, train_df):\n",
        "        # per series: mean squared diff over nonzero trimmed history\n",
        "        y = train_df.filter(like=\"d_\").values\n",
        "        den = np.array([np.mean(np.diff(np.trim_zeros(row))**2) for row in y])\n",
        "        den[~np.isfinite(den)] = np.nan\n",
        "        return den  # length = n_series [2]\n",
        "\n",
        "    def _weights(self, sales_train_end_week):\n",
        "        # compute revenue weights using last 28 days prices * sales at each level\n",
        "        # join sell_prices on wm_yr_wk, then aggregate revenue shares\n",
        "        # return aligned weights per series across all 12 levels\n",
        "        # Placeholder signature; see notebook refs for full detail. [2][1]\n",
        "        pass\n",
        "\n",
        "    def score(self, y_true_df, y_pred_df, train_df):\n",
        "        # y_true_df, y_pred_df: same index/keys as sales_df and 28 d_ cols\n",
        "        # Compute per-level RMSSE then weighted sum\n",
        "        results = []\n",
        "        for keys in self.levels:\n",
        "            # align frames at level\n",
        "            if keys == [\"all\"]:\n",
        "                yt = y_true_df.filter(like=\"d_\").sum(axis=0).to_frame().T\n",
        "                yp = y_pred_df.filter(like=\"d_\").sum(axis=0).to_frame().T\n",
        "                tr = train_df.filter(like=\"d_\").sum(axis=0).to_frame().T\n",
        "                den = np.mean(np.diff(tr.values.squeeze())**2)  # scalar\n",
        "                rmsse = np.sqrt(np.mean((yt.values - yp.values)**2) / den)\n",
        "                w = 1.0  # replace with revenue-based weight\n",
        "                results.append(w * rmsse)\n",
        "            else:\n",
        "                yt = y_true_df.groupby(keys).sum(numeric_only=True)\n",
        "                yp = y_pred_df.groupby(keys).sum(numeric_only=True).reindex(yt.index)\n",
        "                tr = train_df.groupby(keys).sum(numeric_only=True).reindex(yt.index)\n",
        "                den = np.array([np.mean(np.diff(r)**2) for r in tr.filter(like=\"d_\").values])\n",
        "                num = np.mean((yt.filter(like=\"d_\").values - yp.filter(like=\"d_\").values)**2, axis=1)\n",
        "                rmsse = np.sqrt(num / den)\n",
        "                w = np.ones_like(rmsse)  # replace with revenue-based weights\n",
        "                results.append(np.nansum(w * rmsse) / np.nansum(w))\n",
        "        return float(np.mean(results))"
      ],
      "metadata": {
        "id": "Wg3LQc8QAULm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_features(df, max_window=900):\n",
        "    # df: long or wide converted to long per SKU-store with columns:\n",
        "    # ['id','item_id','dept_id','cat_id','store_id','state_id','date','d','sales','price',\n",
        "    #  'dow','month','snap','event_type', ...]\n",
        "    df = df.sort_values(['item_id','store_id','date'])\n",
        "    # target lags\n",
        "    for L in [1, 7, 28]:\n",
        "        df[f\"lag_{L}\"] = df.groupby(['item_id','store_id'])['sales'].shift(L)\n",
        "    # rolling on lag_28 to prevent leakage\n",
        "    grp = df.groupby(['item_id','store_id'])\n",
        "    for W in [7, 28, 56]:\n",
        "        df[f\"rmean_{W}\"] = grp['lag_28'].transform(lambda s: s.rolling(W, min_periods=1).mean())\n",
        "        df[f\"rstd_{W}\"]  = grp['lag_28'].transform(lambda s: s.rolling(W, min_periods=1).std())\n",
        "    # price features\n",
        "    df['log_price'] = np.log(df['price'].replace(0, np.nan)).fillna(method='ffill')\n",
        "    df['price_rolmin_8w'] = grp['price'].transform(lambda s: s.rolling(8, min_periods=1).min())\n",
        "    df['promo_flag'] = (df['price'] <= 1.05*df['price_rolmin_8w']).astype(np.int8)\n",
        "    # compact calendar\n",
        "    df['dow_cat'] = df['dow'].astype('int8')\n",
        "    df['month_cat'] = df['month'].astype('int8')\n",
        "    # drop rows with insufficient history for lags\n",
        "    return df.dropna(subset=['lag_28']).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "_mEzJv9NAYg6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "\n",
        "BASE_PARAMS = dict(\n",
        "    objective=\"poisson\",  # count-like targets; consider 'regression' if negative vals appear\n",
        "    learning_rate=0.05,\n",
        "    num_leaves=31,\n",
        "    max_depth=6,\n",
        "    min_data_in_leaf=50,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    n_estimators=5000,\n",
        "    reg_alpha=0.1,\n",
        "    reg_lambda=0.1,\n",
        "    verbosity=-1\n",
        ")\n",
        "\n",
        "CAT_COLS = ['item_id','dept_id','cat_id','store_id','state_id','dow_cat','month_cat']\n",
        "\n",
        "def train_per_horizon(train_df, valid_df, feature_cols, target_cols):\n",
        "    boosters = []\n",
        "    for h, tgt in enumerate(target_cols, start=1):  # e.g., y_t+1 ... y_t+28 columns\n",
        "        dtrain = lgb.Dataset(train_df[feature_cols], label=train_df[tgt], categorical_feature=CAT_COLS, free_raw_data=False)\n",
        "        dvalid = lgb.Dataset(valid_df[feature_cols], label=valid_df[tgt], categorical_feature=CAT_COLS, free_raw_data=False)\n",
        "        booster = lgb.train(\n",
        "            params=BASE_PARAMS,\n",
        "            train_set=dtrain,\n",
        "            valid_sets=[dvalid],\n",
        "            valid_names=[f\"val_h{h}\"],\n",
        "            num_boost_round=5000,\n",
        "            early_stopping_rounds=200,\n",
        "            verbose_eval=False\n",
        "        )\n",
        "        boosters.append(booster)\n",
        "    return boosters\n",
        "\n",
        "def predict_per_horizon(models, df, feature_cols):\n",
        "    preds = []\n",
        "    for booster in models:\n",
        "        preds.append(booster.predict(df[feature_cols], num_iteration=booster.best_iteration))\n",
        "    return np.stack(preds, axis=1)"
      ],
      "metadata": {
        "id": "WpOWSwbkAdib"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q_PARAMS = dict(objective=\"quantile\", learning_rate=0.05, num_leaves=31, max_depth=6,\n",
        "                n_estimators=3000, subsample=0.8, colsample_bytree=0.8, min_data_in_leaf=50)\n",
        "\n",
        "def train_quantiles(X_tr, y_tr, X_va, y_va, cat_cols, alphas=(0.1,0.5,0.9)):\n",
        "    models = {}\n",
        "    for a in alphas:\n",
        "        mdl = lgb.LGBMRegressor(**Q_PARAMS, alpha=a)\n",
        "        mdl.fit(X_tr, y_tr, eval_set=[(X_va, y_va)], categorical_feature=cat_cols,\n",
        "                eval_metric=\"quantile\", verbose=False)\n",
        "        models[a] = mdl\n",
        "    return models\n",
        "\n",
        "def predict_quantiles(models, X):\n",
        "    preds = {a: m.predict(X) for a, m in models.items()}\n",
        "    # enforce monotone order\n",
        "    import numpy as np\n",
        "    p10, p50, p90 = preds[0.1], preds[0.5], preds[0.9]\n",
        "    p50 = np.maximum(np.minimum(p50, p90), p10)\n",
        "    p10 = np.minimum(p10, p50); p90 = np.maximum(p90, p50)\n",
        "    return p10, p50, p90\n",
        "# bootstrap_pi.py\n",
        "import numpy as np\n",
        "\n",
        "def bootstrap_intervals(point_forecasts, residuals, quantiles=(0.1,0.9), n_boot=200, seed=42):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    B, H = n_boot, point_forecasts.shape[21]\n",
        "    draws = rng.choice(residuals, size=(B, H), replace=True)\n",
        "    sims = point_forecasts[np.newaxis, :, :] + draws[:, np.newaxis, :]  # broadcast per series if needed\n",
        "    lower = np.quantile(sims, quantiles, axis=0)\n",
        "    upper = np.quantile(sims, quantiles[21], axis=0)\n",
        "    return lower, upper"
      ],
      "metadata": {
        "id": "wx3dm9L4Ahpa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def toggle_promo(features, on=True):\n",
        "    feat = features.copy()\n",
        "    feat['promo_flag'] = np.where(on, 1, 0).astype(np.int8)\n",
        "    return feat\n",
        "\n",
        "def toggle_snap(features, state_col='state_id', target_state='CA', on=True):\n",
        "    feat = features.copy()\n",
        "    mask = feat[state_col] == target_state\n",
        "    feat.loc[mask, 'snap'] = 1 if on else 0\n",
        "    return feat\n",
        "\n",
        "def shift_price(features, pct=-0.10):\n",
        "    feat = features.copy()\n",
        "    feat['log_price'] = np.log(np.exp(feat['log_price']) * (1.0 + pct))\n",
        "    # recompute promo if available\n",
        "    if 'price_rolmin_8w' in feat:\n",
        "        price = np.exp(feat['log_price'])\n",
        "        feat['promo_flag'] = (price <= 1.05*feat['price_rolmin_8w']).astype(np.int8)\n",
        "    return feat\n",
        "\n",
        "def score_scenario(model, feat_baseline, transform_fn):\n",
        "    X0 = feat_baseline.copy()\n",
        "    y0 = model.predict(X0)\n",
        "    X1 = transform_fn(X0)\n",
        "    y1 = model.predict(X1)\n",
        "    return y1 - y0"
      ],
      "metadata": {
        "id": "DXId05P1AlXK"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}